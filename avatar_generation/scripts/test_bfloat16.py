"""æµ‹è¯•ä½¿ç”¨bfloat16 - å®˜æ–¹æ¨èé…ç½® + å¤šæ ·åŒ–æç¤ºè¯ç”Ÿæˆ"""

import os
import time
from pathlib import Path

import torch
from diffusers import ZImagePipeline
from tqdm import tqdm

# å¯¼å…¥æç¤ºè¯ç”Ÿæˆå™¨
from diverse_prompt_generator import DiversePromptGenerator

# æµ‹è¯•é…ç½®
NUM_IMAGES = 20
MODEL_PATH = "ckpts/Tongyi-MAI/Z-Image-Turbo"
OUTPUT_DIR = Path("test_avatars_diverse")
OUTPUT_DIR.mkdir(exist_ok=True)

# 1024x1024é…ç½®ï¼ˆå®˜æ–¹æ¨èï¼‰
HEIGHT = 1024
WIDTH = 1024
NUM_INFERENCE_STEPS = 9
GUIDANCE_SCALE = 0.0

# åˆ›å»ºæç¤ºè¯ç”Ÿæˆå™¨
prompt_generator = DiversePromptGenerator(seed=42)


def generate_diverse_prompt(index: int) -> str:
    """ä½¿ç”¨ç”Ÿæˆå™¨åˆ›å»ºå¤šæ ·åŒ–æç¤ºè¯"""
    return prompt_generator.generate_prompt(index)


def main():
    print("=" * 80)
    print("æµ‹è¯•bfloat16 - å¤šæ ·åŒ–ç¾å›½å¥³æ€§å¤´åƒç”Ÿæˆ")
    print("=" * 80)

    # é€‰æ‹©è®¾å¤‡
    if torch.cuda.is_available():
        device = "cuda"
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"âœ“ GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: CPU")

    # åŠ è½½pipeline - ä½¿ç”¨å®˜æ–¹æ¨èçš„bfloat16é…ç½®
    print(f"\næ­£åœ¨åŠ è½½pipelineä»: {MODEL_PATH}")
    print("âœ“ ä½¿ç”¨å®˜æ–¹æ¨èé…ç½®: torch.bfloat16 + low_cpu_mem_usage=False")
    load_start = time.time()

    # å®Œå…¨æŒ‰ç…§README.mdç¤ºä¾‹é…ç½®
    pipe = ZImagePipeline.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,  # å®˜æ–¹æ¨è
        low_cpu_mem_usage=False,     # å®˜æ–¹ç¤ºä¾‹ä¸­çš„é…ç½®
        trust_remote_code=True,
    )
    pipe.to(device)

    load_time = time.time() - load_start
    print(f"âœ“ PipelineåŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}ç§’)")

    # ä¸ä½¿ç”¨CPU offload - æµ‹è¯•åœ¨24GB VRAMä¸Šæ˜¯å¦èƒ½ç›´æ¥è¿è¡Œ
    print("\næµ‹è¯•é…ç½®:")
    print("  - dtype: bfloat16")
    print("  - CPU offload: å…³é—­")
    print("  - åˆ†è¾¨ç‡: 1024x1024")
    print("  - æç¤ºè¯: åŠ¨æ€ç”Ÿæˆï¼ˆç¾å›½å¥³æ€§ï¼Œ30%çƒŸç†å¦†ï¼Œ40%ä¸°æ»¡èº«æï¼Œ25%çº¹èº«ï¼‰")
    print("  - åœºæ™¯: 8ç§ç”Ÿæ´»åŒ–åœºæ™¯éšæœºç»„åˆ")

    # ç”Ÿæˆå›¾åƒ
    print(f"\nå¼€å§‹ç”Ÿæˆ {NUM_IMAGES} å¼ å›¾åƒ ({HEIGHT}x{WIDTH})...")
    print("=" * 80)

    generation_times = []
    success_count = 0

    for i in tqdm(range(NUM_IMAGES), desc="ç”Ÿæˆè¿›åº¦"):
        prompt = generate_diverse_prompt(i)
        seed = 42 + i
        generator = torch.Generator(device).manual_seed(seed)
        output_path = OUTPUT_DIR / f"test_avatar_{i:02d}.png"

        try:
            start_time = time.time()

            images = pipe(
                prompt=prompt,
                height=HEIGHT,
                width=WIDTH,
                num_inference_steps=NUM_INFERENCE_STEPS,
                guidance_scale=GUIDANCE_SCALE,
                generator=generator,
            ).images

            elapsed = time.time() - start_time
            generation_times.append(elapsed)

            # ä¿å­˜å›¾åƒ
            images[0].save(output_path)
            success_count += 1

            # æ‰“å°å‰3å¼ çš„æç¤ºè¯
            if i < 3:
                print(f"\n[å›¾åƒ {i}] ç”ŸæˆæˆåŠŸ")
                print(f"  æç¤ºè¯: {prompt[:80]}...")
                print(f"  ç”Ÿæˆæ—¶é—´: {elapsed:.2f}ç§’")

        except Exception as e:
            print(f"\nâœ— é”™è¯¯ (å›¾åƒ {i}): {e}")
            continue

    # ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("âœ“ ç”Ÿæˆå®Œæˆ!")
    print("=" * 80)

    if generation_times:
        avg_time = sum(generation_times) / len(generation_times)
        min_time = min(generation_times)
        max_time = max(generation_times)
        total_time = sum(generation_times)

        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æˆåŠŸç”Ÿæˆ: {success_count}/{NUM_IMAGES} å¼ ")
        print(f"  æ€»è€—æ—¶: {total_time:.2f} ç§’ ({total_time/60:.2f} åˆ†é’Ÿ)")
        print(f"  å¹³å‡é€Ÿåº¦: {avg_time:.2f} ç§’/å¼ ")
        print(f"  æœ€å¿«: {min_time:.2f} ç§’")
        print(f"  æœ€æ…¢: {max_time:.2f} ç§’")

        # é¢„ä¼°3000å¼ çš„æ—¶é—´
        estimated_3000 = avg_time * 3000
        print(f"\nğŸ“ˆ é¢„ä¼°ç”Ÿæˆ3000å¼ æ‰€éœ€æ—¶é—´:")
        print(f"  å•GPU: {estimated_3000/60:.1f} åˆ†é’Ÿ ({estimated_3000/3600:.2f} å°æ—¶)")
        print(f"  6ä¸ªGPUå¹¶è¡Œ: {estimated_3000/60/6:.1f} åˆ†é’Ÿ ({estimated_3000/3600/6:.2f} å°æ—¶)")

    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR.absolute()}")
    print(f"   æŸ¥çœ‹å›¾åƒ: ls {OUTPUT_DIR}/")
    print("=" * 80)


if __name__ == "__main__":
    main()
